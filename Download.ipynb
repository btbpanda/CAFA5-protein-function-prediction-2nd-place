{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is used to download artifacts produced by `CAFA5Pipeline.ipynb`. Here each paragraph exactly maps the `CAFA5Pipeline.ipynb` numeration. So execution of the step here will be equivalent to execute the corresponding cells to calculate  it from skratch. If some paragraphs are missing, that means, computations are easy and fast, so no need to store the results in cloud storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Get external data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir temporal\n",
    "!mkdir temporal/labels\n",
    "\n",
    "!wget https://storage.yandexcloud.net/cafa5embeds/temporal/cafa-terms-diff.tsv -O temporal/cafa-terms-diff.tsv\n",
    "!wget https://storage.yandexcloud.net/cafa5embeds/temporal/prop_quickgo51.tsv -O temporal/prop_quickgo51.tsv\n",
    "    \n",
    "!wget https://storage.yandexcloud.net/cafa5embeds/temporal/labels/prop_test_leak_no_dup.tsv -O temporal/labels/prop_test_leak_no_dup.tsv\n",
    "!wget https://storage.yandexcloud.net/cafa5embeds/temporal/labels/prop_test_no_kaggle.tsv -O temporal/labels/prop_test_no_kaggle.tsv\n",
    "!wget https://storage.yandexcloud.net/cafa5embeds/temporal/labels/prop_train_no_kaggle.tsv -O temporal/labels/prop_train_no_kaggle.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embeddings\n",
    "\n",
    "### 2.1 T5 pretrained inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir embeds\n",
    "!mkdir embeds/t5\n",
    "\n",
    "for file in ['train_embeds.npy', 'test_embeds.npy', 'train_ids.npy', 'test_ids.npy']:\n",
    "    !wget https://storage.yandexcloud.net/cafa5embeds/embeds/t5/{file} -O embeds/t5/{file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ESM pretrained inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir embeds\n",
    "!mkdir embeds/esm_small\n",
    "\n",
    "for file in ['train_embeds.npy', 'test_embeds.npy', 'train_ids.npy', 'test_ids.npy']:\n",
    "    !wget https://storage.yandexcloud.net/cafa5embeds/embeds/esm_small/{file} -O embeds/esm_small/{file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models\n",
    "files = [f'model_{x}.pkl' for x in range(5)] + ['oof_pred.pkl', 'test_pred.pkl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train and inference py-boost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['pb_t54500_cond', 'pb_t54500_raw', 'pb_t5esm4500_cond', 'pb_t5esm4500_raw']:\n",
    "    !mkdir models/{model}\n",
    "    for file in files:\n",
    "        !wget https://storage.yandexcloud.net/cafa5embeds/boostpreds/{model}/{file} -O models/{model}/{file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train and inference logreg models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['lin_t5_cond', 'lin_t5_raw']:\n",
    "    !mkdir models/{model}\n",
    "    for file in files:\n",
    "        !wget https://storage.yandexcloud.net/cafa5embeds/linpreds/{model}/{file} -O models/{model}/{file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Train and inference NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models/nn_serg\n",
    "for i in range(12):\n",
    "    for j in range(5):\n",
    "        !wget https://storage.yandexcloud.net/cafa5embeds/nn_models_upd/model_{i}_{j}.pt -O models/nn_serg/model_{i}_{j}.pt\n",
    "            \n",
    "!wget https://storage.yandexcloud.net/cafa5embeds/nn_models_upd/pytorch-keras-etc-3-blend-cafa-metric-etc.pkl -O models/nn_serg/pytorch-keras-etc-3-blend-cafa-metric-etc.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final model\n",
    "\n",
    "### 4.1. Train GCN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models/gcn\n",
    "\n",
    "for ns in ['bp', 'mf', 'cc']:\n",
    "    !mkdir models/gcn/{ns}\n",
    "    !wget https://storage.yandexcloud.net/cafa5embeds/gcn/{ns}/checkpoint.pth -O models/gcn/{ns}/checkpoint.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Inference GCN models and TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    !wget https://storage.yandexcloud.net/cafa5embeds/gcn/pred_tta_{i}.tsv -O models/gcn/pred_tta_{i}.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
