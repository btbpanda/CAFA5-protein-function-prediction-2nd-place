{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "Running this notebook end-to-end will reproduce the solution. Step by step guide is also provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat config.yaml\n",
    "\n",
    "with open('config.yaml') as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "    \n",
    "BASE_PATH = CONFIG['base_path']\n",
    "CONFIG_PATH = os.path.join(BASE_PATH, 'config.yaml')\n",
    "RAPIDS_ENV = os.path.join(BASE_PATH, CONFIG['rapids-env'])\n",
    "PYTORCH_ENV = os.path.join(BASE_PATH, CONFIG['pytorch-env'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "### 1.1. Setup envs\n",
    "\n",
    "Create the following python envs:\n",
    "\n",
    "    1) `pytorch-env` - env to deal with all DL models\n",
    "    2) `rapids-env`  - env to preprocess via RAPIDS and train py-boost and logregs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./create-rapids-env.sh {BASE_PATH}\n",
    "!./create-pytorch-env.sh {BASE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Get the input data\n",
    "\n",
    "Here we describe what should be stored in the working dir to reproduce the results\n",
    "\n",
    "Following data scheme was provided by Kaggle:\n",
    "\n",
    "    ./Train - cafa train data\n",
    "    ./Test (targets) - cafa test data\n",
    "    ./sample_submission.tsv - cafa ssub\n",
    "    ./IA.txt - cafa IA\n",
    "\n",
    "    \n",
    "Following are the solution code libraries, scipts, and notebooks used for training:\n",
    "\n",
    "    ./protlib\n",
    "    ./protnn\n",
    "    ./nn_solution\n",
    "    \n",
    "And the installed envs\n",
    "\n",
    "    ./pytorch-env\n",
    "    ./rapids-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Produce the helpers data\n",
    "\n",
    "First, we made some preprocessing of the input data to store everything in format that is convinient to us to handle and manipulate. Here is the structure:\n",
    "\n",
    "    ./helpers\n",
    "        ./fasta - fasta files stored as feather\n",
    "            ./train_seq.feather\n",
    "            ./test_seq.feather\n",
    "        ./real_targets - targets stored as n_proteins x n_terms parquet containing 0/1/NaN values\n",
    "            ./biological_process\n",
    "                ./part_0.parquet\n",
    "                ...\n",
    "                ./part_14.parquet\n",
    "                ./nulls.pkl - NaN rate of each term\n",
    "                ./priors.pkl - prior mean of each term (excluding NaN cells, like np.nanmean)\n",
    "            ./cellular_component\n",
    "            ./molecular_function\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# parse fasta files and save as feather\n",
    "!{RAPIDS_ENV} protlib/scripts/parse_fasta.py \\\n",
    "    --config-path {CONFIG_PATH}\n",
    "\n",
    "# convert targets to parquet and calculate priors\n",
    "!{RAPIDS_ENV} protlib/scripts/create_helpers.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --batch-size 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Get external data\n",
    "\n",
    "Datasets downloaded from outside:\n",
    "\n",
    "    ./temporal - extra data downloaded from http://ftp.ebi.ac.uk/pub/databases/GO/goa/old/UNIPROT/\n",
    "    \n",
    "First step is downloading and parsing the datasets. After parsing, script will separate the datasets by the evidence codes. The most important split for us is kaggle/no-kaggle split. We refer `kaggle` as experimental codes, `no-kaggle` as electornic labeling, that will be used as features for the stacker models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download external data from ebi.ac.uk\n",
    "!{RAPIDS_ENV} protlib/scripts/downloads/dw_goant.py \\\n",
    "    --config-path {CONFIG_PATH}\n",
    "\n",
    "# # parse the files\n",
    "!{RAPIDS_ENV} protlib/scripts/parse_go_single.py \\\n",
    "    --file goa_uniprot_all.gaf.216.gz \\\n",
    "    --config-path {CONFIG_PATH}\n",
    "\n",
    "!{RAPIDS_ENV} protlib/scripts/parse_go_single.py \\\n",
    "    --file goa_uniprot_all.gaf.214.gz \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --output old214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is propagation. Since ebi.ac datasets contains the labeling without propagation, we will apply the rules provided in organizer's repo to labeling more terms. We will do it only for `goa_uniprot_all.gaf.216.gz` datasets since it is the actual dataset at the active competition phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = BASE_PATH + '/temporal'\n",
    "\n",
    "for file in glob.glob(folder + '/labels/train*') + glob.glob(folder + '/labels/test*'):\n",
    "    name = folder + '/labels/prop_' + file.split('/')[-1]\n",
    "\n",
    "    !{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/prop_tsv.py \\\n",
    "        --path {file} \\\n",
    "        --graph {BASE_PATH}/Train/go-basic.obo \\\n",
    "        --output {name} \\\n",
    "        --device 0 \\\n",
    "        --batch_size 30000 \\\n",
    "        --batch_inner 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part is reproducing MT's datasets that are commonly used in all public kernels. We didn't use it directly, but we used `cafa-terms-diff` dataset, that represents the difference between our labeling obtained by parsing `goa_uniprot_all.gaf.216.gz` dataset and `all_dict.pkl` dataset given by MT. As he claims in the dicussion [here](https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/discussion/404853#2329935) he used the same FTP source as we. But our source is more actual than the public. So the difference is actually the temporal. After analysis, we find out, that we are able to reproduce it as the difference between `goa_uniprot_all.gaf.216.gz` and `goa_uniprot_all.gaf.214.gz` sources. So, we just create `cafa-terms-diff` dataset by the given script. The only difference between the source in the kaggle script and used here is deduplication. We removed duplicated protein/terms pairs from the dataset, it has almost zero impact on the metric value (less than 1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/reproduce_mt.py \\\n",
    "    --path {BASE_PATH}/temporal \\\n",
    "    --graph {BASE_PATH}/Train/go-basic.obo\n",
    "\n",
    "# # make propagation for quickgo51.tsv\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/prop_tsv.py \\\n",
    "    --path {BASE_PATH}/temporal/quickgo51.tsv \\\n",
    "    --graph {BASE_PATH}/Train/go-basic.obo \\\n",
    "    --output {BASE_PATH}/temporal/prop_quickgo51.tsv \\\n",
    "    --device 0 \\\n",
    "    --batch_size 30000 \\\n",
    "    --batch_inner 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Preparation step for neural networks\n",
    "\n",
    "Produce some helpers to train NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/prepare.py \\\n",
    "    --config-path {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 T5 pretrained inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/t5.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ESM pretrained inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/esm2sm.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train and inference py-boost models\n",
    "\n",
    "GBDT models description:\n",
    "\n",
    "1) Features: T5 + taxon, targets: multilabel\n",
    "\n",
    "2) Features: T5 + taxon, targets: conditional\n",
    "\n",
    "3) Features: T5 + ESM + taxon, targets: multilabel\n",
    "\n",
    "4) Features: T5 + ESM + taxon, targets: conditional\n",
    "\n",
    "Pipeline and hyperparameters are the same for all the models. Target is 4500 output: BP 3000, MF: 1000, CC: 500. All models could be ran in parallel to save a time. We used single V100 32GB and it requires about 15 hours to train 5 fold CV loop for each model type. 32GB GPU RAM is required, otherwise OOM will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name in ['pb_t54500_raw', 'pb_t54500_cond', 'pb_t5esm4500_raw', 'pb_t5esm4500_cond', ]:\n",
    "\n",
    "    print(f'Training {model_name}')\n",
    "\n",
    "    !{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/train_pb.py \\\n",
    "        --config-path {CONFIG_PATH} \\\n",
    "        --model-name {model_name} \\\n",
    "        --device 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train and inference logreg models\n",
    "\n",
    "Logistic Regression models description:\n",
    "\n",
    "1) Features: T5 + taxon, targets: multilabel\n",
    "\n",
    "2) Features: T5 + taxon, targets: conditional\n",
    "\n",
    "\n",
    "Pipeline and hyperparameters are the same for all the models. Target is 13500 output: BP 10000, MF: 2000, CC: 1500. All models could be ran in parallel to save a time. We used single V100 32GB and it requires about 10 hours for model 1 and 2 hours for model 2 to train 5 fold CV loop. 32GB GPU RAM is required, otherwise OOM will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['lin_t5_raw', 'lin_t54500_cond']:\n",
    "\n",
    "    print(f'Training {model_name}')\n",
    "\n",
    "    !{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/train_lin.py \\\n",
    "        --config-path {CONFIG_PATH} \\\n",
    "        --model-name {model_name} \\\n",
    "        --device 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Train and inference NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create train folds (the same as used for pb_t54500_cond model)\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/create_gkf.py \\\n",
    "    --config-path {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/train_models.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/inference_models.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/make_pkl.py \\\n",
    "    --config-path {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final model\n",
    "\n",
    "### 4.1. Train GCN models\n",
    "\n",
    "This step is training 3 independent stacking models for each ontology. Models are trained on single V100 GPU and it takes about 13 hours for BP, 4 hours for MF and 2 hours for CC. 32 GB GPU RAM is required to fit. Could be trained in parallel if 2 GPUs are avaliable - BP and MF/CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ont in ['bp', 'mf', 'cc']:\n",
    "    !{PYTORCH_ENV} {BASE_PATH}/protnn/scripts/train_gcn.py \\\n",
    "        --config-path {CONFIG_PATH} \\\n",
    "        --ontology {ont} \\\n",
    "        --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Inference GCN models and TTA\n",
    "\n",
    "Inference and Test-Time-Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!{PYTORCH_ENV} {BASE_PATH}/protnn/scripts/predict_gcn.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Postprocessing and build submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have 4 TTA predictions, we need to aggregate all as an average\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/collect_ttas.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0\n",
    "\n",
    "# create 0.3 * pred + 0.7 * max children propagation\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/step.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0 \\\n",
    "    --batch_size 30000 \\\n",
    "    --batch_inner 3000 \\\n",
    "    --lr 0.7 \\\n",
    "    --direction min\n",
    "\n",
    "# create 0.3 * pred + 0.7 * min parents propagation\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/step.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0 \\\n",
    "    --batch_size 30000 \\\n",
    "    --batch_inner 3000 \\\n",
    "    --lr 0.7 \\\n",
    "    --direction max\n",
    "\n",
    "# here we average min prop and max prop solutions, mix with cafa-terms-diff and quickgo51 datasets from 1.4\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/make_submission.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0 \\\n",
    "    --max-rate 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {BASE_PATH}/sub/submission.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
